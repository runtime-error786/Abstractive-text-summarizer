{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q\n",
    "! pip install --upgrade accelerate\n",
    "! pip uninstall -y transformers accelerate\n",
    "! pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from datasets import load_dataset, load_from_disk, load_metric\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"google/pegasus-cnn_dailymail\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset_samsum = load_dataset(\"samsum\",trust_remote_code=True)\n",
    "\n",
    "print(dataset_samsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n",
    "\n",
    "    return {\n",
    "        'input_ids' : input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_samsum_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset_samsum_pt[\"train\"]\n",
    "\n",
    "print(train_data[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print(train_data[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='pegasus-samsum', \n",
    "    num_train_epochs=1, \n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=1, \n",
    "    per_device_eval_batch_size=1,\n",
    "    weight_decay=0.01, \n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps', \n",
    "    eval_steps=500,  \n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_pegasus,  \n",
    "    args=trainer_args,\n",
    "    tokenizer=tokenizer,  \n",
    "    data_collator=seq2seq_data_collator,  \n",
    "    train_dataset=dataset_samsum_pt[\"train\"],  \n",
    "    eval_dataset=dataset_samsum_pt[\"validation\"]  \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model_pegasus.save_pretrained(os.path.join(\"pegasus-samsum-model\"))\n",
    "tokenizer.save_pretrained(os.path.join(\"tokenizer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "        \"\"\"\n",
    "        split the dataset into smaller batches that we can process simultaneously\n",
    "        Yield successive batch-sized chunks from list_of_elements.\n",
    "        \"\"\"\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i : i + batch_size]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_metric_on_test_ds(self, dataset, metric, model, tokenizer,\n",
    "                                batch_size=16, device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "                                column_text=\"article\",\n",
    "                                column_summary=\"highlights\"):\n",
    "        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "        for article_batch, target_batch in tqdm(\n",
    "            zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "            inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                            padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "            summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                            attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                            length_penalty=0.8, num_beams=8, max_length=128)\n",
    "            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=True)\n",
    "                for s in summaries]\n",
    "\n",
    "            decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "            metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "        score = metric.compute()\n",
    "        return score\n",
    "\n",
    "def evaluation(self):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)\n",
    "        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)\n",
    "       \n",
    "        dataset_samsum_pt = load_from_disk(self.config.data_path)\n",
    "        rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
    "        rouge_metric = load_metric('rouge')\n",
    "        score = self.calculate_metric_on_test_ds(\n",
    "        dataset_samsum_pt['test'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary')\n",
    "\n",
    "        rouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\n",
    "\n",
    "        return rouge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import pipeline\n",
    "\n",
    "def predict(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(r'C:\\Users\\musta\\OneDrive\\Desktop\\Abstractive-text summarizer\\tokenizer')\n",
    "    model = r'C:\\Users\\musta\\OneDrive\\Desktop\\Abstractive-text summarizer\\pegasus-samsum-model'\n",
    "\n",
    "    kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 50}\n",
    "\n",
    "    Pipeline = pipeline(\"summarization\", model= model, tokenizer=tokenizer)\n",
    "\n",
    "    summary = Pipeline(text, **kwargs)[0][\"summary_text\"]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "In today's fast-paced world, the importance of time management cannot be overstated. \n",
    "As we juggle multiple responsibilities, effectively managing our time allows us to prioritize tasks, reduce stress, \n",
    "and increase productivity. Good time management involves planning, setting clear goals, and allocating appropriate time to each task. \n",
    "Many people struggle with procrastination, but overcoming this challenge requires discipline and focus. \n",
    "By being mindful of how we spend our time, we can achieve a better work-life balance and pursue our long-term goals with greater efficiency.\n",
    "\"\"\"\n",
    "\n",
    "summary = predict(sample_text)\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
